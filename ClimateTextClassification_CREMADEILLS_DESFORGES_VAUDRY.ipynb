{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmftMgX7p-uG"
   },
   "source": [
    "# Climate change Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PuE5i4NHp7a6",
    "outputId": "0a4128b5-a59c-47ff-8566-65669e57e10b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import urllib.request # Import the urllib.request module\n",
    "\n",
    "# URL of the JSONL file\n",
    "url = \"https://www.sustainablefinance.uzh.ch/dam/jcr:df02e448-baa1-4db8-921a-58507be4838e/climate-fever-dataset-r1.jsonl\"\n",
    "\n",
    "# Function to read the JSONL file line by line\n",
    "def read_jsonl_from_url(url):\n",
    "    data = []\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        for line in response:\n",
    "            data.append(json.loads(line.decode('utf-8')))\n",
    "    return data\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "df = pd.DataFrame(read_jsonl_from_url(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "5DajH0WK2bkV",
    "outputId": "aec8dd2f-2b88-4d1b-a07f-9acc98f085c7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_id</th>\n",
       "      <th>claim</th>\n",
       "      <th>claim_label</th>\n",
       "      <th>evidences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Global warming is driving polar bears toward e...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[{'evidence_id': 'Extinction risk from global ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>The sun has gone into ‘lockdown’ which could c...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[{'evidence_id': 'Famine:386', 'evidence_label...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>The polar bear population has been growing.</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[{'evidence_id': 'Polar bear:1332', 'evidence_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>Ironic' study finds more CO2 has slightly cool...</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[{'evidence_id': 'Atmosphere of Mars:131', 'ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>Human additions of CO2 are in the margin of er...</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[{'evidence_id': 'Carbon dioxide in Earth's at...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  claim_id                                              claim claim_label  \\\n",
       "0        0  Global warming is driving polar bears toward e...    SUPPORTS   \n",
       "1        5  The sun has gone into ‘lockdown’ which could c...    SUPPORTS   \n",
       "2        6        The polar bear population has been growing.     REFUTES   \n",
       "3        9  Ironic' study finds more CO2 has slightly cool...     REFUTES   \n",
       "4       10  Human additions of CO2 are in the margin of er...     REFUTES   \n",
       "\n",
       "                                           evidences  \n",
       "0  [{'evidence_id': 'Extinction risk from global ...  \n",
       "1  [{'evidence_id': 'Famine:386', 'evidence_label...  \n",
       "2  [{'evidence_id': 'Polar bear:1332', 'evidence_...  \n",
       "3  [{'evidence_id': 'Atmosphere of Mars:131', 'ev...  \n",
       "4  [{'evidence_id': 'Carbon dioxide in Earth's at...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_T11CuxA2ZPG",
    "outputId": "e7d64bb3-6436-4b59-ec56-4f3169bd6c87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       claim_id                                              claim  \\\n",
      "count      1535                                               1535   \n",
      "unique     1535                                               1535   \n",
      "top        3134  Over the last decade, heatwaves are five times...   \n",
      "freq          1                                                  1   \n",
      "\n",
      "       claim_label                                          evidences  \n",
      "count         1535                                               1535  \n",
      "unique           4                                               1534  \n",
      "top       SUPPORTS  [{'evidence_id': 'Greenland ice sheet:43', 'ev...  \n",
      "freq           654                                                  2  \n"
     ]
    }
   ],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7El4JogsqVk7",
    "outputId": "657d9844-df48-412a-8461-d77c5d160448"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1535, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "gMhrGR6D3YmX",
    "outputId": "882bef1d-5805-4e05-d047-601399ec5da1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "claim_label\n",
       "SUPPORTS           654\n",
       "NOT_ENOUGH_INFO    474\n",
       "REFUTES            253\n",
       "DISPUTED           154\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.value_counts(\"claim_label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbL2VVdtWKg0"
   },
   "source": [
    "#Start your project here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 : Divide the data in a balanced train set (85%) and a validation set (15%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YxFgZ8wBamDf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportions dans l'ensemble d'entraînement:\n",
      "claim_label\n",
      "SUPPORTS           0.425727\n",
      "NOT_ENOUGH_INFO    0.308576\n",
      "REFUTES            0.165391\n",
      "DISPUTED           0.100306\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Proportions dans l'ensemble de validation:\n",
      "claim_label\n",
      "SUPPORTS           0.427948\n",
      "NOT_ENOUGH_INFO    0.310044\n",
      "REFUTES            0.161572\n",
      "DISPUTED           0.100437\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Proportion pour l'ensemble de validation\n",
    "val_ratio = 0.15\n",
    "\n",
    "# Répartition équilibrée\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "\n",
    "for label in df[\"claim_label\"].unique():\n",
    "    class_indices = df[df[\"claim_label\"] == label].index.values\n",
    "    np.random.shuffle(class_indices)\n",
    "    val_size = int(len(class_indices) * val_ratio)\n",
    "    val_indices.extend(class_indices[:val_size])\n",
    "    train_indices.extend(class_indices[val_size:])\n",
    "\n",
    "train_df = df.loc[train_indices].reset_index(drop=True)\n",
    "val_df = df.loc[val_indices].reset_index(drop=True)\n",
    "\n",
    "print(\"Proportions dans l'ensemble d'entraînement:\")\n",
    "print(train_df[\"claim_label\"].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nProportions dans l'ensemble de validation:\")\n",
    "print(val_df[\"claim_label\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 : train a simple RNN model to predict column “claim_label” as a function of column “claim” without too much overfitting. \n",
    "> You will not have enough time nor data to create a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IPSA\\Aero5\\2.14_ma512_Deep_Neural_Network_et_Deep_Learning\\projet\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 44ms/step - accuracy: 0.3310 - loss: 1.3575 - val_accuracy: 0.4279 - val_loss: 1.2601\n",
      "Epoch 2/20\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.4167 - loss: 1.2700 - val_accuracy: 0.4279 - val_loss: 1.2513\n",
      "Epoch 3/20\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.4378 - loss: 1.2472 - val_accuracy: 0.4192 - val_loss: 1.3437\n",
      "Epoch 4/20\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.5741 - loss: 1.1173 - val_accuracy: 0.4061 - val_loss: 1.3910\n",
      "Epoch 5/20\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.6545 - loss: 0.9224 - val_accuracy: 0.4105 - val_loss: 1.4712\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Vérification de la disponibilité du GPU\n",
    "print(\"GPU disponible :\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Optionnel : Limiter la mémoire GPU si nécessaire\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Alloue dynamiquement la mémoire GPU\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "# Préparation des données\n",
    "label_mapping = {label: idx for idx, label in enumerate(train_df[\"claim_label\"].unique())}\n",
    "train_df[\"claim_label\"] = train_df[\"claim_label\"].map(label_mapping)\n",
    "val_df[\"claim_label\"] = val_df[\"claim_label\"].map(label_mapping)\n",
    "\n",
    "# Tokenisation et padding\n",
    "vocab_size = 10000\n",
    "max_length = 100\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_df[\"claim\"])\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df[\"claim\"])\n",
    "val_sequences = tokenizer.texts_to_sequences(val_df[\"claim\"])\n",
    "\n",
    "train_padded = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=max_length, padding=\"post\")\n",
    "val_padded = tf.keras.preprocessing.sequence.pad_sequences(val_sequences, maxlen=max_length, padding=\"post\")\n",
    "\n",
    "train_labels = train_df[\"claim_label\"].values\n",
    "val_labels = val_df[\"claim_label\"].values\n",
    "\n",
    "# Construction du modèle\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 16, input_length=max_length),\n",
    "    tf.keras.layers.SimpleRNN(32, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.SimpleRNN(16),\n",
    "    tf.keras.layers.Dense(len(label_mapping), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(train_padded, train_labels, validation_data=(val_padded, val_labels), epochs=20, batch_size=32, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage des performances du modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 42.7948%\n"
     ]
    }
   ],
   "source": [
    "# Évaluation de l'accuracy sur l'ensemble de validation\n",
    "val_loss, val_accuracy = model.evaluate(val_padded, val_labels, verbose=0)\n",
    "print(f\"Validation Accuracy: {100*val_accuracy:.4f}%\")\n",
    "#print(f\"Validation Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 : fine-tune (https://huggingface.co/docs/transformers/training) a pretrained model from huggingface.co, in two slightly different ways, to predict column “claim_label” as a function of column “claim” without too much overfitting. \n",
    "> You will not have enough time nor data to create a test set. You will need to tokenize your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Charger le tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokeniser les données\n",
    "def tokenize_function(claim):\n",
    "    return tokenizer(\n",
    "        claim,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"tf\",\n",
    "    )\n",
    "\n",
    "# Appliquer la tokenisation à chaque texte dans la colonne \"claim\"\n",
    "train_encodings = train_df[\"claim\"].apply(tokenize_function)\n",
    "val_encodings = val_df[\"claim\"].apply(tokenize_function)\n",
    "\n",
    "# Convertir les encodages en format utilisable par TensorFlow (dict de tenseurs)\n",
    "def format_encodings(encodings):\n",
    "    return {key: tf.constant([enc[key].numpy() for enc in encodings]) for key in encodings[0]}\n",
    "\n",
    "train_encodings = format_encodings(train_encodings)\n",
    "val_encodings = format_encodings(val_encodings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\IPSA\\Aero5\\2.14_ma512_Deep_Neural_Network_et_Deep_Learning\\projet\\.venv\\Lib\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\IPSA\\Aero5\\2.14_ma512_Deep_Neural_Network_et_Deep_Learning\\projet\\.venv\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "41/41 [==============================] - 598s 14s/step - loss: 1.3796 - accuracy: 0.3691 - val_loss: 1.3086 - val_accuracy: 0.3100\n"
     ]
    }
   ],
   "source": [
    "# Convertir les labels en tenseurs\n",
    "train_labels = tf.convert_to_tensor(train_df[\"claim_label\"].values, dtype=tf.int32)\n",
    "val_labels = tf.convert_to_tensor(val_df[\"claim_label\"].values, dtype=tf.int32)\n",
    "\n",
    "# Charger un modèle pré-entraîné pour la classification\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(train_df[\"claim_label\"].unique())\n",
    ")\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(\n",
    "    optimizer=\"adam\",  # Utilisation du nom directement\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Entraîner le modèle\n",
    "    with tf.device('/GPU:0'):\n",
    "        # Placez ici le code d'entraînement du modèle\n",
    "        history = model.fit(\n",
    "            {\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"]},\n",
    "            train_labels,\n",
    "            validation_data=(\n",
    "                {\"input_ids\": val_encodings[\"input_ids\"], \"attention_mask\": val_encodings[\"attention_mask\"]},\n",
    "                val_labels\n",
    "            ),\n",
    "            epochs=20,  # Vous pouvez augmenter pour plus d'entraînement\n",
    "            batch_size=32\n",
    "        )\n",
    "except:\n",
    "    history = model.fit(\n",
    "            {\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"]},\n",
    "            train_labels,\n",
    "            validation_data=(\n",
    "                {\"input_ids\": val_encodings[\"input_ids\"], \"attention_mask\": val_encodings[\"attention_mask\"]},\n",
    "                val_labels\n",
    "            ),\n",
    "            epochs=5,  # Vous pouvez augmenter pour plus d'entraînement\n",
    "            batch_size=32\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geler les couches de base (le modèle pré-entraîné)\n",
    "for layer in model.layers[:-1]:  # Gèle toutes les couches sauf la dernière\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compiler à nouveau le modèle\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Entraîner le modèle\n",
    "history_partial_fine_tuning = model.fit(\n",
    "    {\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"]},\n",
    "    train_labels,\n",
    "    validation_data=(\n",
    "        {\"input_ids\": val_encodings[\"input_ids\"], \"attention_mask\": val_encodings[\"attention_mask\"]},\n",
    "        val_labels\n",
    "    ),\n",
    "    epochs=3,  # Limité pour éviter l'overfitting\n",
    "    batch_size=16\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
